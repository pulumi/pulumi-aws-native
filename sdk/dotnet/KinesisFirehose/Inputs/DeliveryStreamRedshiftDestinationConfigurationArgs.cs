// *** WARNING: this file was generated by pulumi. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.AwsNative.KinesisFirehose.Inputs
{

    public sealed class DeliveryStreamRedshiftDestinationConfigurationArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// The CloudWatch logging options for your delivery stream.
        /// </summary>
        [Input("cloudWatchLoggingOptions")]
        public Input<Inputs.DeliveryStreamCloudWatchLoggingOptionsArgs>? CloudWatchLoggingOptions { get; set; }

        /// <summary>
        /// The connection string that Kinesis Data Firehose uses to connect to the Amazon Redshift cluster.
        /// </summary>
        [Input("clusterJdbcurl", required: true)]
        public Input<string> ClusterJdbcurl { get; set; } = null!;

        /// <summary>
        /// Configures the Amazon Redshift `COPY` command that Kinesis Data Firehose uses to load data into the cluster from the Amazon S3 bucket.
        /// </summary>
        [Input("copyCommand", required: true)]
        public Input<Inputs.DeliveryStreamCopyCommandArgs> CopyCommand { get; set; } = null!;

        /// <summary>
        /// The password for the Amazon Redshift user that you specified in the `Username` property.
        /// </summary>
        [Input("password", required: true)]
        public Input<string> Password { get; set; } = null!;

        /// <summary>
        /// The data processing configuration for the Kinesis Data Firehose delivery stream.
        /// </summary>
        [Input("processingConfiguration")]
        public Input<Inputs.DeliveryStreamProcessingConfigurationArgs>? ProcessingConfiguration { get; set; }

        /// <summary>
        /// The retry behavior in case Firehose is unable to deliver documents to Amazon Redshift. Default value is 3600 (60 minutes).
        /// </summary>
        [Input("retryOptions")]
        public Input<Inputs.DeliveryStreamRedshiftRetryOptionsArgs>? RetryOptions { get; set; }

        /// <summary>
        /// The ARN of the AWS Identity and Access Management (IAM) role that grants Kinesis Data Firehose access to your Amazon S3 bucket and AWS KMS (if you enable data encryption). For more information, see [Grant Kinesis Data Firehose Access to an Amazon Redshift Destination](https://docs.aws.amazon.com/firehose/latest/dev/controlling-access.html#using-iam-rs) in the *Amazon Kinesis Data Firehose Developer Guide* .
        /// </summary>
        [Input("roleArn", required: true)]
        public Input<string> RoleArn { get; set; } = null!;

        /// <summary>
        /// The configuration for backup in Amazon S3.
        /// </summary>
        [Input("s3BackupConfiguration")]
        public Input<Inputs.DeliveryStreamS3DestinationConfigurationArgs>? S3BackupConfiguration { get; set; }

        /// <summary>
        /// The Amazon S3 backup mode. After you create a delivery stream, you can update it to enable Amazon S3 backup if it is disabled. If backup is enabled, you can't update the delivery stream to disable it.
        /// </summary>
        [Input("s3BackupMode")]
        public Input<Pulumi.AwsNative.KinesisFirehose.DeliveryStreamRedshiftDestinationConfigurationS3BackupMode>? S3BackupMode { get; set; }

        /// <summary>
        /// The S3 bucket where Kinesis Data Firehose first delivers data. After the data is in the bucket, Kinesis Data Firehose uses the `COPY` command to load the data into the Amazon Redshift cluster. For the Amazon S3 bucket's compression format, don't specify `SNAPPY` or `ZIP` because the Amazon Redshift `COPY` command doesn't support them.
        /// </summary>
        [Input("s3Configuration", required: true)]
        public Input<Inputs.DeliveryStreamS3DestinationConfigurationArgs> S3Configuration { get; set; } = null!;

        /// <summary>
        /// The Amazon Redshift user that has permission to access the Amazon Redshift cluster. This user must have `INSERT` privileges for copying data from the Amazon S3 bucket to the cluster.
        /// </summary>
        [Input("username", required: true)]
        public Input<string> Username { get; set; } = null!;

        public DeliveryStreamRedshiftDestinationConfigurationArgs()
        {
        }
        public static new DeliveryStreamRedshiftDestinationConfigurationArgs Empty => new DeliveryStreamRedshiftDestinationConfigurationArgs();
    }
}
