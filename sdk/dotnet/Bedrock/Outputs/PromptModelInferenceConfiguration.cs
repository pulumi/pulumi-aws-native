// *** WARNING: this file was generated by pulumi. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.AwsNative.Bedrock.Outputs
{

    /// <summary>
    /// Prompt model inference configuration
    /// </summary>
    [OutputType]
    public sealed class PromptModelInferenceConfiguration
    {
        /// <summary>
        /// Maximum length of output
        /// </summary>
        public readonly double? MaxTokens;
        /// <summary>
        /// List of stop sequences
        /// </summary>
        public readonly ImmutableArray<string> StopSequences;
        /// <summary>
        /// Controls randomness, higher values increase diversity
        /// </summary>
        public readonly double? Temperature;
        /// <summary>
        /// Cumulative probability cutoff for token selection
        /// </summary>
        public readonly double? TopP;

        [OutputConstructor]
        private PromptModelInferenceConfiguration(
            double? maxTokens,

            ImmutableArray<string> stopSequences,

            double? temperature,

            double? topP)
        {
            MaxTokens = maxTokens;
            StopSequences = stopSequences;
            Temperature = temperature;
            TopP = topP;
        }
    }
}
