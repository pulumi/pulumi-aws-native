// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.AwsNative.Bedrock.Outputs
{

    /// <summary>
    /// BasePromptConfiguration per Prompt Type.
    /// </summary>
    [OutputType]
    public sealed class AgentPromptConfiguration
    {
        /// <summary>
        /// If the Converse or ConverseStream operations support the model, `additionalModelRequestFields` contains additional inference parameters, beyond the base set of inference parameters in the `inferenceConfiguration` field.
        /// 
        /// For more information, see [Inference request parameters and response fields for foundation models](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html) .
        /// </summary>
        public readonly Outputs.AgentAdditionalModelRequestFields? AdditionalModelRequestFields;
        /// <summary>
        /// Base Prompt Template.
        /// </summary>
        public readonly string? BasePromptTemplate;
        /// <summary>
        /// The agent's foundation model.
        /// </summary>
        public readonly string? FoundationModel;
        /// <summary>
        /// Contains inference parameters to use when the agent invokes a foundation model in the part of the agent sequence defined by the `promptType` . For more information, see [Inference parameters for foundation models](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html) .
        /// </summary>
        public readonly Outputs.AgentInferenceConfiguration? InferenceConfiguration;
        /// <summary>
        /// Specifies whether to override the default parser Lambda function when parsing the raw foundation model output in the part of the agent sequence defined by the `promptType` . If you set the field as `OVERRIDDEN` , the `overrideLambda` field in the [PromptOverrideConfiguration](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_PromptOverrideConfiguration.html) must be specified with the ARN of a Lambda function.
        /// </summary>
        public readonly Pulumi.AwsNative.Bedrock.AgentCreationMode? ParserMode;
        /// <summary>
        /// Specifies whether to override the default prompt template for this `promptType` . Set this value to `OVERRIDDEN` to use the prompt that you provide in the `basePromptTemplate` . If you leave it as `DEFAULT` , the agent uses a default prompt template.
        /// </summary>
        public readonly Pulumi.AwsNative.Bedrock.AgentCreationMode? PromptCreationMode;
        /// <summary>
        /// Specifies whether to allow the inline agent to carry out the step specified in the `promptType` . If you set this value to `DISABLED` , the agent skips that step. The default state for each `promptType` is as follows.
        /// 
        /// - `PRE_PROCESSING` – `ENABLED`
        /// - `ORCHESTRATION` – `ENABLED`
        /// - `KNOWLEDGE_BASE_RESPONSE_GENERATION` – `ENABLED`
        /// - `POST_PROCESSING` – `DISABLED`
        /// </summary>
        public readonly Pulumi.AwsNative.Bedrock.AgentPromptState? PromptState;
        /// <summary>
        /// The step in the agent sequence that this prompt configuration applies to.
        /// </summary>
        public readonly Pulumi.AwsNative.Bedrock.AgentPromptType? PromptType;

        [OutputConstructor]
        private AgentPromptConfiguration(
            Outputs.AgentAdditionalModelRequestFields? additionalModelRequestFields,

            string? basePromptTemplate,

            string? foundationModel,

            Outputs.AgentInferenceConfiguration? inferenceConfiguration,

            Pulumi.AwsNative.Bedrock.AgentCreationMode? parserMode,

            Pulumi.AwsNative.Bedrock.AgentCreationMode? promptCreationMode,

            Pulumi.AwsNative.Bedrock.AgentPromptState? promptState,

            Pulumi.AwsNative.Bedrock.AgentPromptType? promptType)
        {
            AdditionalModelRequestFields = additionalModelRequestFields;
            BasePromptTemplate = basePromptTemplate;
            FoundationModel = foundationModel;
            InferenceConfiguration = inferenceConfiguration;
            ParserMode = parserMode;
            PromptCreationMode = promptCreationMode;
            PromptState = promptState;
            PromptType = promptType;
        }
    }
}
